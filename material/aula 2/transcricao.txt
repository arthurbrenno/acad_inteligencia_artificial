A gente não recebe uma palavra, a gente recebe zero e um, tá? Então desmistifica o negócio, pelo amor de Deus, tá? Imagina aí, se eu fizer uma cortana aqui, pronto, pessoal, pega a chave, né? Dá pra fazer uma coisa parecida, mas não dá nem pra outra. Então, pessoal, a estrutura básica do neurônio é essa daqui, tem as suas entradas, chamada camada de entrada, tem os seus pesos neurais, tem a camada intermediária e a camada de saída, que nós vamos chamar de net. Bom, pessoal, aí tu me chama de Y, tá? Que é a minha camada de saída. Eu tenho o quê aqui na estrutura? Eu tenho o somatório, esse somatório ficou feio pra caramba, né? Eu tenho o somatório e tenho a minha função de ativação. Assim como o neurônio, quando ele recebe as informações, tem que ter um M.I.A. onde esse neurônio vai disparar. É a mesma ideia. Quem desenvolveu esse neurônio foi um... dois autores chamados Markham e outro chamado Pits. Aí virou o neurônio Markham e Pits, tá ok? Um era psicólogo e o outro era matemático. Aí juntou o psicólogo e o matemático, e aí sai a coisa boa, né? Ok? Então, desenvolvemos o quê? Esse carinha. Esse de cá é tranquilo porque ele faz a somatória das entradas e esse carinha aqui é chamado de função de ativação. Essa primeira função que nós vamos fazer hoje, tá? Essa primeira função, o quê que ela vem a ser? Ela é chamada de função de grau. É a mais simples que existe. Eu vou usar ela depois? Não, porque é tão besco que não dá pra usar muitas coisas, não. É uma função que fala o seguinte. Se a minha net for maior ou igual a zero, a saída dela é 1. Se a net for menor que zero, a saída dela é zero. Tá ok? Essa é uma função de ativação. Tem funções de ativação piores? Tem. Função de ativação significa que você tem que fazer derivada dela pra achar como é que você vai jogar dentro da programação. Tá ok? Essa é a mais simplesinha. Função de grau, ativação de grau. Tem aquela... Isso, agora depois eu lembro do nome. Então, essa função é tão simplesinha que é só um curso usar. Mas, essa aqui é a primeira, tá? A gente vai ficar utilizando a mais simples. E ela, isso vai fazer o quê? Ela vai pegar a net que vai receber aí a função de ativação dela. Bom, o que que acontece? Pessoal, como que esse neurônio vai aprender? Se vocês observarem aqui a minha estrutura, pessoal, isso aqui representa pra mim uma função de primeira ordem, tá? Aonde o W1 e o W2 correspondem ao ângulo dessa reta e o Y, esse WB, ele representa o quê? Se ela vai subir ou se ela vai descer, tá? Essa primeira linha de neurônio lá dá mais zero que uma reta. Uma curva de reta AX mais B, tá? Isso aqui, ó. AX mais B. Aí o que que é o grande pulo dela? Qual é o segredo dela? É o quê? O algoritmo de aprendizagem que foi desenvolvido pra ela treinar. Esse foi o grande jogada de tudo, né? Então como que funciona esse algoritmo? Ele trabalha com o quê? Com uma chamada derivada de decaimento. Pessoal, a maioria desses neurônios fazem isso, aprendem com isso, aprendem, tá? A maioria deles usa a derivada de decaimento. Como que ela funciona? Ele vai procurando, na função, o menor ponto. Quando ela acha o menor ponto, a minha estrutura, eu sei que o meu neurônio aprendeu. Por quê? Chegou no fosso, vamos falar assim, né? Pode acontecer daí que ele saia e ir embora? Pode. A gente neurônio emburrece, tá? Se ela treinar muito, ela emburrece. Tem isso também. Tem que fazer um gerenciamento pra ver o quanto que ela vai aprender e não vai esquecer. Essa jogadinha também. Se treinar muito, ela vai embora e acaba que... Se ela está acertando tudo, é tudo errado. Então tem todo um algoritmo de gerenciamento dela. Você pega uma PI, né? Passinha, que você só coloca duas linhas lá e dá um ok, e ela faz isso aqui tudo por dentro. O cara já viu eu fazer isso lá. Bom, o que que acontece? A derivada de decaimento é dada pelo quê? A variação do meu peso menos esse zeta é chamado de taxa de aprendizagem, tá? Essa taxa do dentro da derivada, o pessoal adotou-se 0,6. Por quê, Floresval? 0,6? Me prova, não tem jeito. Foi feito de forma empírica. O cara foi chutando até achar o valor. Tá? Você vai achar em nenhum lugar que falou matematicamente que é 0,6. Tá? O pessoal adotou-se 0,6. E eu tenho o quê? Eu tenho que achar a variança do meu erro pela variação dos pesos. Tá? Aí que vem a ideia, né? Que o Margot Didi desenvolveu matemática de 0,6. Falou assim, ó, então o que que eu tenho que achar? Meus pesos neurais que eu tenho que achar, eu tenho que vir variando... Pra que eu pensei assim, pessoal? Eu escrevi variação, derivada. Eu tenho que variar esses valores para que eu reduza o meu erro numa velocidade igual a 0,6 e eu venha achar onde o meu neurônio vai aprender. Certo? Pois, ó, então qual é o erro que foi usado aqui? É chamado de erro quadrático médio. Erro quadrático médio é o valor do target de notar o valor que eu quero que meu neurônio saiba menos o que saiu nele, que é a net, certo? O que é esse target? Quando eu vou fazer um treinamento, eu falo pra ele assim, ó, se entrar 0 em 0, eu quero que saia 0. Se entrar 0 em 1, eu quero que saia 1. Se entrar 1 em 0, eu quero que saia 0. Se entrar 1 em 1, eu quero que saia 1. Entendeu? O target é o valor que eu desejo. Pois, ó, então pra treinar uma rede neural, eu tenho que saber quais são as variáveis de entrada e quais são as desejadas? Tem, né? Não vai adivinhar. Ok? Por isso que eu falo pra vocês, pessoal, rede neural não gera nada de novo. E a não gera nada de novo. Por quê? Ela tem que ser ensinada. Tá ok? Por ela ser ensinada, ela vai fazer simplesmente uma cópia do que você fez na treinada. Ou ela pode até melhorar essa cópia, mas gerar uma coisa nova, vocês esquecem. Tá? Então, ó, beleza. Então, eu quadrado, ó, treinador quadrado, médio dividido por 2. Essa fórmula aqui é 1. Não, se mudar essa fórmula, muda tudo? Muda. Tá? Essa é a coisa mais basiquinha delas. E o que que é minha net, Floresval? O que que é minha net? Eu só venho aqui, ó. A minha net, nesse caso aqui, o que que ela vai ser? Ela é somatória, ó, do quê? De menos 1, vez WB, mais X1, vez W1, mais X2, vez W2. Floresval, por que que ele é menos 1? Por que que o Baia, você falou que é menos 1? Depende, você estuda. Lá na UFSCar, que eu fiz uma matéria de rede neural uma vez, com o Ivan, e lá eles adotam positivo. Na UFSCar, eles adotam negativo. E eu acredito que é negativo. Porque é um dos dois que eu vou usar mais cedo. Tá? Tem um motivo de, talvez acelere alguma coisa? Não. Tá? Essa aí é a padronização dele, tá? Então tem essas duas funções. Só o que que acontece com o que tem um BOzinho aqui? O que que é o BO que tem? Tem que fazer a derivada do meu erro em função de W. Só que esse cara não tem W, tá vendo? Ele não tem W. Aqui embaixo tem, ó. O que que eu faço? O famoso ajuste técnico. Multiplico por net, e divido por net. Se eu multiplico e divido, eu mudo a função? Não. Não. Então se eu reescrever essa fórmula assim, ó. Ó. Agora eu forcei. O que que eu forcei aqui? O WE. Derivada de erro por net, ó. Tem um net? Não tem? E a derivada de net por? Da outra W. Então ajustei. Tá? Então o que que é a derivada disso aqui? Eu vou fazer aqui, pessoal. Não é o sistema de derivada, pessoal. Eu vou fazer aqui rapidinho. Faço só o... Isso aqui não vai cair na prova, não. Repito mais uma vez, tá? Ó. A função que veio. Net ao quadrado. Dividido por dois. Só fiz a distribuição do polimônio, tá? Fiz a derivada agora, ó. Que tá aqui, lembra aí? Tudo junto aqui, né? Que é isso. Né? Tem o quadrado, não é isso? Então vai ficar o que, ó? A derivada desse cara. Como é em função de net, esse cara já vale zero, né? Aqui vai ficar menos dois target mais dois net dividido por dois. Corta, corta, corta. Então eu tenho que a derivada desse em função de net é menos target mais net. Pois volta. Então peraí. Se mudar a função, mudar alguma coisa aqui, vai mudar esse equacionamento tudo? O algoritmo que eu vou usar vai ficar diferente? Vai. Muda tudo. Agora eu descrevo, tá? Você tem ideia? Ah, aqui faz reconhecimento de... de caracter, né? Linguagem natural, a ativação é completa. O erro tem completa indiferença ali. Muda tudo. Tem que aproximar tudo de novo. Tem. Tá? E você pega uns artigos aí que saem, né, fresquinhos aí hoje, a pessoa não vai tirar isso aqui machigado. Ela vai só usar a função. Você se vira pra achar. Ninguém vai tirar e fazer coisa de graça lá. Tá? Tá bom. Tem isso aqui. O que eu preciso mais? A derivada do net em função de W. Eu vou escolher só um aqui, só esse carinha aqui, ó. Vou chamar ele, ó, que é a minha derivada de net, vai ser Xn. Vai servir pra todo mundo, tá? Pra Wm. Como é uma soma, eu posso separar. Se eu derivar esse aqui em função de W, vai sobrar só o quê? Xn. Beleza. Tá aqui, ó, a derivada. Junto tudo, ó. Então a variação do peso vai ser o quê? Menos o Zeta multiplicado por menos target mais net multiplicado por Xn. O peso, esse K, esse delta, você escreveu ele lá na física, lembra? Em final mês inicial, dado menos dado do zero. Pessoal, esse aqui você escreveu na física. A delta S, lembra? Dividido pela delta T. Esse K significa S menos S zero, T menos T zero. T menos T zero. Né? Ah! Target menos net, Txn, joga pra lá. Ó que legal! Esse aqui, pessoal, pra aprendizagem do meu precepto, é a função que eu utilizo pra aprender. Então essa briga toda aí, pessoal, foi pra fazer o quê? Achar essa equaçãozinha que eu vou usar lá pras fórmulas, que eu vou precisar dela. Entendido? Então é daí que saiu a brincadeira, ó. Pessoal, vou dar uma prova assim. Prove, matematicamente, que é a função de aprendizagem no meu precepto é tal. Pessoal, quer ler? Prova. A prova é só isso. É uma questão valendo 20 pontos. Beleza? Vocês vão ver, pessoal, que rei de neural é só cálculo. Cálculo, cálculo, cálculo, cálculo. E multiplicação de matrix. Tá? A prova. Eu achei a conta tão interessante, mano, depois que vocês vão montar na biblioteca de vocês, não vai morrer o conhecimento de imagens. Vocês vão ver o tanto que pesa esse negócio. Beleza? O que que eu vou fazer? Eu vou capturar a tela de vocês, vou abrir o software, vou ir falando qual linguagem faz o que que faz, a que vocês quiserem. Por quê? Vocês que vão montar a biblioteca de vocês. Conselho de amigo, use uma linguagem a qual você já tem feeling, né? Afinidade. Não tem nenhuma igual o cara ali, ele usa a bombinha? Não tem nenhuma, né? Né? Você só gosta de vomite, é verdade? Esqueceu de aplicar nas pernas, mas tudo bem. Tá ok? Então, o que que acontece? Ih, não, está parecido. Perfeito, então. Beleza? Vou capturar vocês, abro o serrado aqui, vou fazer um passo a passo, aconselho de prestar atenção. Quem quiser, né? Né? Eu vou fazer o serrado. É isso mesmo, né? Aí vocês estão pensando assim, nossa, o que que eu percebo que é o mais fácil? É assim, você imagina. Mas é mais ou menos isso mesmo, tá? É. É. Tem que ir para frente, só para trás. Só que a vantagem é o que? Se você entendesse certinho, você compreende muito mais fácil do que os outros, tá? Até o do site FPT, esse dito texto, a estrutura dele é bem parecida com isso aqui, a ideia é bem parecida. Ah, muda pra caramba, muda, só que assim, no I.O. você foi ver algumas coisas. Mudar para o discreto. Muda mesmo, muda para o áudio. É, mudou. Mas é o discreto. É o discreto, vai dar uma mudança. Eu demorei para fazer... Eu tenho aqui, vou usar o modo que você viu rodando. Eu demorei... O que? O meu ficou bugado. Pronto. Você estava fazendo o ar cheio nessa parte. Não, não, não. É verdade. Vamos lá, ó. Abri aqui o Super SciLAB. O que é que eu uso o SciLAB, pessoal? Primeiro, olha só o diretor aí. Segundo, ele tem várias ferramentas de I.A. Várias ferramentas. Prontas, como você pode, por exemplo, montar uma rede neural e fazer a parte de reconhecimento de imagem, pra você fechar isso tem que fazer outra. E a gente pode fazer várias aplicações. Lembra que você estava ouvindo. Só que o mesmo homem que eu. Então, vamos lá, pessoal. Como é que vai funcionar isso aqui? No SciLAB, ele tem esse ambiente de... Esse ambiente aqui e tal. Você tem que fechar aqui. Se não abrir, quem vai mexer com o SciLAB, né? Você vem aqui no SciNotes, tá, ó. Que vai abrir isso aqui pra você. Aqui você vai entrar na programação e de cá você faz a... Os resultados das suas operações. Quando eles falam, ó, mas o que é que você usa essa porcaria? É o seguinte, pessoal. Quando eu vou fazer meus softwares, eu uso muito o Java, tá. O que que eu prefiro fazer? Eu prefiro fazer no SciLAB. Por quê? Também nessa aba de cá, tudo que é feito no programa, ele me mostra todas as variáveis, em tempo real, o que que aconteceu. Então, se tiver algum erro, eu consigo identificar muito mais fácil se eu fosse fazer no Java, que só seria uma menina pra mim lá, avisando, ó, você errou aqui, babaca. Vai arrumar um cachá depois, né. Então, eu prefiro usar o SciLAB, porque tudo que eu faço, eu desenvolvo tudo no SciLAB e depois eu transcrevo para a linguagem que eu quero. Ok? Mas se você quiser fazer direto na linguagem, se você gosta, à vontade. Tem linguagem muito boa pra trabalhar, que é a Lua, né, Ruby, que são boas também pra trabalhar. O Lua eu gosto, porque dá pra aplicar em jogos, que mexe com Lua ali, né. Não sei. Então, tem umas linguagens aí que são muito legais pra trabalhar isso. Bom, então vamos lá. Primeira coisa que eu faço no SciLAB. O primeiro, pessoal, limpo todas as, esse clear, tá, ele dá um clear geral no meu ambiente. Limpo tudo que tiver avaliado. Eu não, vai que uma vai acionar aqui alguma coisa que não pode, né. E esse MC é pra limpar justamente essa tela, ó, se eu dar um exemplo, tá, ó, dessa unha, eu só vejo, ah, não aceitou. Arquivos, eu não sei porque colocaram eu como júnior aqui até agora, sabe. Sim, beleza, sim. Ó, limpo, entendeu? Então, vamos lá. O que que eu preciso saber, primeiramente, pra fazer um perceptor na unha? Eu preciso dos dados de entendimento. Esses dados, o que que eu vou ter aqui, ó, o menos um que é o bias, correto? Zero, zero, que é o x1 e o x2. E o target é zero, certo? Então quando for zero e zero, eu quero que saia zero. Quando for zero e um, eu quero que saia zero. Quando for um e zero, eu quero que saia zero. E quando for um e um, eu quero que saia um. Então isso aqui é uma porta o quê? A porta end. Beleza? Então eu criei aqui meus dados. Ó que legal, montou um arquivo, pessoal, facinho, ó. Vou colocar aqui dados agora, ó. Viu que legal? Já aparece aqui pra mim, ó. Tem uma visão também, ó. Posso ver tudo que ele está fazendo. Bom, então o que que acontece aqui agora? Eu vou ter que criar meus pesos neurais, que são os dados. Então eu vou escrever assim, ó, criando os pesos neurais. O peso neural, né, vou criar aqui um vetor. O peso neural, o que que acontece? Ele é um valor aleatório de zero a um. Tem que ser aleatório, tá? Ok? Então eu vou fazer aqui, como são três neurônios, tá? Eu tenho três aqui. Então eu vou fazer aqui, ó, i vai de um até três, são três entradas, correto? Eu posso colocar isso aqui se eu quiser também, tá, pessoal? Se eu vier aqui, ó, vai ficar mais... Vou colocar aqui, ó, número de entradas, vai ser o quê? É... Size... dados... dois... menos um. Por quê? Se ele me dá o número de coroas da matriz, correto? Ele também, ó, posso usar outro comando também, ó, número de amostras. Size... Por que eu tô fazendo assim? Porque se eu alterar os dados, ele já muda pra mim sozinho aqui, entendeu? Então eu preciso ficar vigiando, né, essas coisas aí, ok? Ele já lança pra mim tudo o que eu preciso. Pode ser o número de entrada em meio. Aqui? Esse aqui significa coluna, esse aqui significa linhas. Tá? Se eu mandar rodar aqui, ó, aqui, ó, no cantinho aqui, ó, o número de amostras que eu tenho. Quatro. Eu tenho quatro linhas de amostras. E o número de entradas eu tenho três. Eu tenho de coroas menos um, porque a última eu sei que é o target. Beleza? E se eu aumentar isso aqui, ele já mexe sozinho o programa, ele já fica alterando a programação. Fica mais, vamos falar assim, automático, né? Beleza. Então o que nós fizemos, ó, dado vai receber quem? I... Eu dou um comando aqui, range... Certo? Vou mostrar pra vocês aqui. Então, ó, meu dado tá aqui, ó. Certo? Por exemplo, ele ficou em uma coluna. É ideal esse identificamento. Tá? Beleza. O que que eu vou fazer agora, ó? Vou fazer a primeira linha. Vou fazer assim, ó, a minha amostra. A minha amostra, vou começar com a primeira, depois eu faço o loop aqui. Vou explicando assim, enquanto eu fecho o loop. Amostra número um, o que que eu vou fazer aqui, ó? Meu X vai ser dados, que é a matriz, dados. Vou pegar a linha da amostra, eu tô analisando, certo? Do um até NE. Certo? Ó, aqui eu tô rodando, eu só posso usar o loop. Se eu apertar X, tá vendo? Menos um, zero, zero. Eu peguei esses três. E o meu target, meu VG, o que que vai ser? Dar dados, dar amostra. Eu sei que é o quê? NE mais um, que é o target, correto? Então, ó, rodou. Opa. Então, ó, meu target tá aqui de zero, e meu X, esse aqui. Beleza? Tranquilo? Bom, o que que eu tenho que fazer aí? Eu tenho que fazer a somatória do produto. Então, lembra o que eu falei pra vocês? A partir de hoje, rede neural é matriz, é multiplicação de matriz. O meu X é o vetor, certo? Só que é uma matriz o quê? De uma linha e três colunas. O meu W, ó, é o quê? É uma matriz de três linhas e uma coluna. Você viu que eu posso fazer a multiplicação, ó? Assim, certo? Eu posso fazer aqui, ó, que a minha net, o que que vai ser a minha net? X que multiplica e tem? W. Quanta facilidade? Aqui eu já vou fazer a somatória dos produtos, ó. A soma da rodada, ó, net, ó o resultado aqui, ó. Vai ser zero, não vai? Que é a função de ativação. Vou habilitar a função de ativação. Vamos ver lá. Função... Função de ativação. O povo tá rindo, tá feliz, né? É, desisteu. Não, isso. Função de ativação. Como vira a função de ativação? Se net for maior ou igual a zero, eu volto pro quadro. Então, net vai ser o quê? Um. Se não, net, igual a zero. Ó, essa é minha função de ativação. Então, ó, se eu mandar rodar aqui, ó, net, ó, saiu um zero. Quer dizer, quando for zero, os erros são zero, não é? Beleza. O que eu faço agora? Calculo o erro. Normalmente o erro a gente chama de delta, tá? Adesão. Se quiser saber erro também, fica à vontade. O que que é o meu erro? É o target menos o quê? O que saiu lá? O net. O que mais? Só a net. Eu tenho um erro. Vou naquela equação agora. Fazer a equação. Isso aqui, ó. Certo? Esse aqui, o meu, eu chamo ele de divisão, não é isso? Tem que se ver. Eu tenho meu x, certo? Meu x. E vamos fazer a sua multiplicação. Tá? E ele tem que ser só, multiplicado por um, pelo, a taxa de aprendizagem. E eu vou somar com meus pesos antigos. Só que uma observação. O meu x, pra ele ficar assim, na linha, eu tenho que fazer ele ficar na coluna. Por quê? Esse cara tá aí em coluna, não é isso? O W. No sai lá, dá coisa mais difícil fazer isso. Ó. Meu x tá assim, tá? Se eu colocar x aspa, ele faz a transposta. Automático, não é? Não toma dificuldade. Tá? Aspa é transposta. Até se fosse uma matriz, tá? Uma matriz aqui de, por exemplo, 1, 2, 3... Puta que pariu. H é igual a, 1, 2, 3, 4, 5, cada cê morre. Certo? Se eu quiser fazer a transposta, o H é transposta. Se eu quiser fazer a transposta, se eu quiser fazer a determinante desse cara, a determinante, que se passa a fazer o raiva lá no cálculo, 2, 3, tá bom. Tá? Por isso que eu vou fazer 5, entendeu? Dá pra fazer a inversa também, dá pra fazer a inversa. Qual que sofreu menos? Rapaz, você não podia expedir. Não. Embaçou, meu. Rapaz, mas passou mal que sim. Nem passei raiva. Vamos lá. Então, o que vai acontecer? Meu peso novo, imagina eu ter dw, o que que vai ser? Vai ser a taxa de aprendizagem que é 0,6. Menos o quê? Menos o D, que é o erro. Menos o quê? Dx transposto. Vai ter com essa transposta que a pessoa vai usar até no fim de dias e horários, tá? Beleza? Deu zero. Por que deu zero? Porque com certeza eu entrei no zero do erro. E como é que é a aprendizagem? W vai ser igual ao W antigo mais o quê? O que eu achei. Só isso, gente. Não precisa aprendizar o meu nome. Mais nada. Então, vamos rodar aqui, ó. Beleza. O que que eu faço agora? Vem aqui em cima. A minha amostra não vale 1? Vai ter que fazer o quê? For am, vai de 1 até onde? Até o número de? Amostra. Amostra, que eu tenho. São 4. Ente. Beleza? Então, se eu fazer isso aqui vai rodar 4 vezes. Vou fazer um DISP aqui, ó. DISP. Mostrar, né? W. Vou botar aqui no meu DISP. Pessoal, eu uso a linguagem se vocês quiserem. O que vocês querem fazer, tá? Ó. Primeiro momento. Segundo momento. Ó o peso mudando. Terceiro momento. Quarto momento. A cada loop que vai passando no neurônio, ele vai ajustando os seus pesos neurais. Tá? Por exemplo, tá certo isso aqui? Não, não. O que que eu vou fazer? No loop de amostra, eu vou mudar um outro loop. For chamado de época. Eles falam de época, que eu falo aí. Sabe, pessoal? Isso é padronizado, tá? Época é o número de vezes que eu vou rodar a minha projeção total de amostra que eu tenho. Eu vou rodar isso umas 10 vezes, ó. Ó. E não mudou nada. Ó. Vou botar lá a barriga de novo. Ó, mudou, mudou. Aqui... Tá vendo que não mudou? Porque não teve erro, ó. Ó, mudou. Mudou aqui embaixo, ó. E você vai observando, ó. Mudou aqui. Ó, aqui ficou fixo. Mudou de novo. Pode ir descendo, ó. Você vai ver, vai observar. Ele não está tendo troca mais, ó. Ó. Como é que acabou? Eu entendi dois, eu entendi dois. Ó, ficou. Olha aqui. É, ele aprendeu. Isso. Mas o que ele aprendeu? Ele aprendeu a porta íntima. Olha isso aqui, ó. Ele vai entrar no tempo que for 0 e 0. 0 e 0. Hã? Aí o que vai acontecer? Fomos logo, mas... Que coisa bêbada, né? Eu usava o IF. Com 4, né? Eu já sabia, certeza. Não. Por que isso não vai dar certo? Porque é o seguinte, ó. Pega meus pés aqui, não está? E se você me injetasse na rede? Não... É... Menos 1, colocasse aqui 0.5 e... Ponto... Sei lá. 4. O que é que vai acontecer? A gente vai dar um meio a meio. Não está muito... Não. Vou pegar e buscar por W. Desculpa, ó. Des W. Deu meio. Dá 0 de resposta. Então, a rede, ela generaliza qualquer entrada que tiver de 0 a 1 aí. Então, ela tem meio aporte, tá? End. Só que se você jogar qualquer coisa para ir diferente também, ela vai te dar uma resposta embasada que você instinuou ela. Você colocou 2 e 2 também? Hã? Você colocou 2... Não, é de 0 a 1. A rede está travada de 0 a 1. Tá? Se você pegar lá... Colocar aqui para 0 e 8, por exemplo, ó. Olha lá, ele está mais ou menos 0 ainda. Se eu colocar 8 e 8... Depois eu subi 1, né? Lá, deu 1 já. Deu 0,17, né? É igual a 0 ou superior ou valido. É a resposta. Igual a 0 ou superior ou valido. É a função, ó. Se mexe com o maior ou igual a 0, valido. Quando deu 0,17, valido. Entendeu? Então, é a forma que essa rede vai fazer o quê? Ela vai ajustando e vai aprendendo. Você viu aqui, ó. Esse aqui, pessoal, que é o período de aprendizagem. Meu professor lá da UFRES, o professor Teixe Manaca, fiz um DMP-5. É pós-doutorado de milhares de apontes. Feliz fato. Mas agora, por exemplo, a gente tem um exemplo aqui. A gente está digitalizando livros. Esses livros a gente vai usar para alimentar ela de... Como é que a gente tem que fazer? Porque a gente tem que transformar cada palavra, cada letra em 0 e 1, só que cada letra pode ser 5, 0, 5, 1, 0, 1, 0zinho. 0, 1, 0zinho. 0, 0, 0, 2, 0, 0, 1, 0, 2, 0, 0, 2 e vai embora.
Mas ele tem que sair criando outros neurônios assim pra poder fazer... Isso é mais de saco de palavras. Sem relacionar a palavra... Mas aí como é que você falou também do grupo também? Palavras parecidas com o senhor? É quase uma récord, eles estão na numeração próxima. Mas aí você faz esse próprio cálculo aqui com outra função? É outra, primeiro ele tem o Rio de Janeiro. O algoritmo de organizar suas palavras é outro algoritmo. Aí você só puxa depois ele por meio da Rio de Janeiro. Eu acho que ele também faria essa própria divisão aqui nele pra criar pra você. Normalmente você perde mais tempo organizando os dados pra Reginaldo receber do que ela mesmo. Você pega uma imagem, por exemplo. Você sabe que é matéria de jornalista? Sim. Você é jornalista de margem? Então você sabe mesmo, né? Você não vai fazer aqui do seu lado. O que você vai fazer? Pega a imagem, vai passar por quatro filtros, vai fazer a convolução, vai reduzir, vai fazer o filtro de novo, vai fazer a convolução, vai fazer o vetor daquela imagem. Depois desse vetor você vai injetar na rede neural. Então, até chegar na rede neural, vai o par. Depois vai a rede neural final. Tem que limpar antes pra poder jogar. Tem. Tem que tomar cuidado com isso, entendeu? Então, essas estruturinhas são feitas dessa forma. O que que acontece? Brincadeira aqui. Ah, o que que é a desvantagem dessa rede neural? Esse neurônomo, né? Qual que é a desvantagem dele? Se eu colocar aqui a famosa XOR, que é o O exclusivo, XOR, tá? Vai levar o par de ser impunido. Vou até colocar aqui pra você ver. Pode não. Que burro. Os caras não partem de grau. Os caras? Os caras é foda, hein? Ó, a XOR é esse aqui, correto? Quando é 0 e 1 é 1, quando é 1 e 0 é 1. E uma dá a rodar aqui, ó. Olha que legal. Você tá vendo a finalização ali que tá mudando? Ela leva o rate do processo, ela não aprende. Por que que ela não aprende? Na verdade ela só sabe dois. Seguinte, se eu for... O cara desligou? Desligou, né? Passa o serviço. Sou o 103% então? Hã? Tá apertando? Ele tá só no cutucando. Mas nesse caso aí nem... Fala. Não, se for igual a 0, a diferença é por 1. Como? Se for igual, ela vai colocar 0, a diferença é por 1. Mas ela não vai aprender. Não. Essa aí é difícil. Você tem que fazer um infra aí no caso. Porque eu lembro que foi pra vocês no começo. Não, tem infrão. Não, tem moço nesse caso. Não, tem a resolução. Tem solução pra isso, tá? Foi feito depois de 10 anos. Nossa. Aí, tivesse feito o infra aí na gambiarra, tinha resolvido. Não, é para ajuste técnico. 2 iguais, 4 igual a 0. Isso é muita gambiarra. E o diferente é o número de vida que segue. 2 iguais, 4 igual a 0. Vamos lá. Vamos lá. A 0, 0 é 0. 1 e 1 é 0. Vamos pegar aqui a porta íntima. Ele passou uma linha aqui, pessoal. Ó. Eu vou fazer o algoritmo e mostrar pra vocês o que é agora. Ele passou uma linha aqui, galera. O que que ele falou? Tudo aqui pra cima vale 1. Tudo aqui pra baixo vale 0. A gente fez justamente isso. Se você fosse fazer a porta OR, por exemplo. Ela ia cortar onde? Aqui, não é? Tudo aqui pra cima vale 1. Tudo aqui pra baixo vale 0. O problema do OR exclusivo, por isso as edições elas ficaram paradas 10 anos, foi o quê? Você tem que pegar esse cara aqui e este cara aqui e colocar do mesmo lado. Comecei a separar esses caras com uma reta só. Uma reta só. Bugou. Vamos lá, entendeu? Por isso que faz essa brincadeira. Foi 10 anos. Aí colocaram. Sabe quando o cara tem ideia, explode. Nossa. Aí, de repente, faz um bum. Pegou a carreira elétrica. Ai, bom. É que não. Não, brincadeira. Não, brincadeira. Isso é satanagem. Isso é satanagem. Vamos lá. Então, o que acontece, pessoal? Como que resolveram isso? Vamos passar já uma palhinha pra aproximação. Como é que resolveu isso? Montou uma rede com mais um neurônio. Então, eu tenho dois neurônios. Faz isso aqui, olha. Tudo que está aqui no centro vale 1. Tudo que está externo vale 0. Aí surgiu o quê? O Preceptor Multicamada, que foi o David, que é David Clayton, que desenvolveu esse algoritmo fodidão que é usado até hoje. É chamado de Sistema Depropagation. Propagação para trás, né? Beleza? Pode ser para outro momento. Eu vou fazer aqui, olha, só para vocês verem esse gráfico aí. Vou fazer para vocês verem como é que funciona. Olha. Ah, vou desenhar aqui para vocês verem. Eu vou pegar a função da NET e vou criar uma equação, tá? E dá para fazer só com esse aqui, tá? O neurônio é de quinta dimensão, sexta dimensão, não tem computador nenhum que faz isso. Então, vamos lá. Eu sei que NET é igual, no caso aqui, menos WB, isso aqui é menos 1, né? Mais X1 vezes W1, mais X2 vezes W2, correto? O que eu vou fazer? Eu sei que NET, é a reta, não é isso? Nessa reta, minha NET vale 0, correto? Por quê? Maior ou igual a 0 vale 1, menor que 0 vale 0. Menos WB, mais X1, W1, mais X2, W2. Vamos falar que esse aqui é meu X2 e esse aqui vai ser meu X1, correto? Então, olha, vai ter X1 em função de X2. Então, eu vou pegar e fazer o quê? X1, W1, tem para cá negativo, igual, menos WB, mais X2, W2. Multiplico pelo 1, X1, W1, igual a WB, menos X2 vezes W2. Passa dividindo, X1 é o quê? Menos W2 dividido por W1, vezes X2, mais WB, dividido por W1. É aqui que eu estou falando para vocês, prestem atenção. Esse aqui é uma equação da reta, está vendo aqui? C é igual a AX mais B, está vendo o X aqui? É uma equação da reta. Esse primeiro vira o quê? É uma equaçãozinha da reta. Beleza? Então, é negativo, certo? O que eu vou fazer? Eu, a cada momento aqui, que eu estou mostrando para vocês aqui, eu vou gerar um gráfico aqui. Vou pegar aqui o meu... O meu X está aqui, o meu W está aqui mesmo? Está. Então vai ficar como aqui a brincadeira? Eu vou fazer um for, J, que ele vai varrer, que ele vai varrer essa aqui. Deixa eu criar um ranger aqui. Ranger. Parte zero, 0.2 até... Até 1 está bom, não é? Isso, não é? Está. O que isso aqui faz, pessoal? É o seguinte, olha. Olha que facilidade que é o... Como é que fala? Que é o... Ranger. Olha o que ele faz. Ele saltou de 0.2, 0.2 até 1. Está vendo? Eu vou jogar aqui assim aqui. Certo? Se eu quiser melhorar, eu vou colocar 0.1 e 1. O que ele faz? Está ali um salto. Ranger. 0.1 e 0.9. Então vai ser 11 elementos. Certo? Eu botei aqui. O que eu faço? Então for, J, vai de 1 até 11. Vou criar aqui um grafo, vou chamar de... Xplot. O Xplot aqui. Vai ter algumas posições. E aqui dentro eu vou falar o quê? Que o meu Xplot e... O que vai receber? Vou olhar lá. Menos W2 dividido por W1, né? Esse aqui não. W1 mais WB dividido... WB não, desculpa. W1. Opa, aqui foi errado. Aqui é 2 e aqui é 3. Desculpa. Aqui não tem 0. W1 e WB no W. Dividido por W2. Tá? Você vai votacionar todo mundo aqui. Você vai botar o play no cara. Xplot, ó. Tá meado? Vai ter 11 pontos aqui. Aqui é J. Quê? Multiplicar pelo X2 aí não? Ou não foi aquela... Ah, aí tem que multiplicar mesmo. Obrigado por ter lembrado. Vezes quem? Vezes Ranger, né? J. J. Beleza? Então, X2. Tá variando, né? Xplot. Aí, ó. Beleza? Show de bola? Então, o resultado é aqui, ó. E esse aqui é meu Ranger. Então, vamos fazer aqui, ó. Plot. Ranger. Xplot. Olha lá, retinho. Olha lá isso aí. Não sei o que é. Sei lá. Deixa eu ver. Eu vou colocar aqui agora o que, ó. Um... Sleep. Vamos ver, ó. Olha, olha, olha. As curvinhas dele, ó. Pô. Esse aqui tá Ward ainda? Tá. Aí, fudeu. Vai virar uma bagunça. Vai virar um rolo. Ele tá tentando enxergar, mas não vou adiantar. Ah, não é nenhum, tadinho. Ah, tá. Ele tá só em um. Deixa eu desbloquear aqui. Deixa eu botar no Ranger. Não, aqui eu acabei com ele. Não, pra finalizar isso, então. Na morte dele. Na morte. Não, esse erro aqui, pessoal. É normal, tá? Esse aqui é normal aparecer. Você vem aqui, ó. Não aparecia. Você tem o Minus lá? Mas você não fez isso justamente pra gente fazer com... A porta... A porta Shore, na verdade. Não, a Shore é pra você que não funciona. E aí você viu que não funciona. É, você viu que não funciona. Não funciona mesmo. Não é de nada. Ó. Então isso aí não é o que o cara inventou lá, não. Hã? Isso aí não é o que o cara inventou pra resolver isso dela, não. Por que não? Não, eu tô perguntando. Tá, deixa eu ver aqui. Você não sabe? Rapaz, se você não sabe... Ah, ele tá escondido em algum lugar ali, ó. Aqui, ó. Vem na reta aqui, ó. Aqui, ó. Aqui ele já aprendeu. O que ele falou? Deixa eu... Deixa eu fechar aqui, ó. Tá fazendo sempre a mesma coisa, entendeu? Tá? Deixa eu começar de novo aqui. Na porta. Eu vou colocar aqui... Um... Uma aprendizagem bem pequena pra vocês verem ele movimentar, entendeu? Vou colocar aqui 0.01. Tá? Só pra vocês verem ele movimentar, ó. Tá vendo aqui, ó. Isso aqui, pessoal, quando o neurônio vai aprendendo, tá, ó. Ele vai mexendo nessa estrutura. Vê que ele subiu aqui do 1, ó. 1 e 1. Certo, ó. Aqui, ó. Tá... Aqui é 1 e 0. Aqui é 0 e 0. Tá vendo que ele vai deslocando o neurônio até chegar no 1 e 1 aqui, ó. O 1 e 1 tá aqui mais ou menos, tá vendo, ó. Então ele tá subindo aqui e não tá achando mais erro, tá vendo, ó. Olha lá, ajustou. Ele foi deslocando a reta até chegar no ponto de estabilização. Quando ele parou nesse ponto de estabilização ele aprendeu. Entendeu a jogadinha, ó. Ele vai mexendo, mexendo, mexendo e para aqui no ponto de aprendizado. Beleza? Aí o que aconteceu? Eu vou te finalizar a aula de hoje, tá, ó. O que aconteceu? Cada vez que eu rodo esse algorítico, o jeito que ele tá aí, cada vez que eu rodo esse algorítico, ele vai cair uma reta numa posição diferente. Ele pode cair fazendo assim. Pode cair fazendo assim. Assim. Tá certo, por quê? Você parou pra cima é 1, pra baixo é 0. Aí tem uma proposta que eu peço pra um cara aqui, um dos darinhos mais safo, que ele falou o seguinte, vamos fazer diferente um pouco. Fazer o quê? Vamos ajustar até ficar perfeito isso aí. O que que ele fez? Em vez dele pegar a saída do neurônio aqui, ó, tá. Soma, joga a subestimação e joga pra net aqui. O que que foi a ideia dele? Vamos separar isso aqui. Colocar a função aqui. Colocar a net aqui. E o erro vai ser tirado desse cara aqui. Então meu delta vai ser calculado aqui, ó. Então meu desenho lá daqui é o quê? O tight menos a net sem ter passado pela subestimação. E eu uso pra ensinar meus teus neurais. Eu faço isso. Tá? Tem que mudar muita coisa aqui, pessoal? Não. Não, pessoal, não. O que que eu faço? Tá vendo minha precisação nessa daqui? Eu só simplesmente pego isso aqui e jogo pra antes da subestimação. Só isso. Realoquei a posição. Deixa eu fechar aqui, ó. Mandar rodar de novo. Aí ele vai fazer o quê? Ele tem uma tendência a fazer um posicionamento melhor, vamos falar assim, né. Ele vai ajustando a... Ele vai ficar assim, ó. O objetivo dele é ficar abaixo de 1 aqui, tá vendo? Só que tem que pegar o 1 e o 1, viu? Deixa eu puxar pra cima aqui, ó. O 1 e o 1 tá aqui, tá vendo? Mais ou menos, tá aqui. O 1 e o 1 tá aqui, ó. Tá descendo, ó. Ele tem que ficar abaixo desse 1, ó. Então ele vai fazer um ajuste mais fino, vamos falar assim, tá vendo? Ele vai tentando, tentando... É. Isso chama derivado de decaimento. Ele vai chegar até o erro tender a zero. Ele vai procurando até o erro tender a zero. Tá me entendendo? Nada de... Aqui faltou 1 pra agora, né? Deixa eu aumentar a função dele aqui um pouquinho, ó. Mudar pra 2. Deixa aí 1 mesmo pra você ver o que é o pior. Piorar mais um pouquinho pra você ver. Então, 5. O que eu vou colocar aqui é... Vou colocar aqui 100. E aqui eu vou colocar... Isso aí tá bom. Tá até subindo, ó. Nossa, vamos ver. Esse é bonito, né, pessoal? Não, vai preencher. Vai preencher, porque ele tá subindo, ó. Aí desse jeito aqui diminui o erro ou não dá bem? Não, porque é o erro tender a zero. Eu... Eu disse que eu tenho que cobrir, né? Deixa eu só colocar pra subir de novo aqui pra... Zero, 1. Flores, mas nesse caso é igual o que você falou. Que ela chega um momento e ela fica burra. Nesse sentido, como é que ela ficaria burra? Ela vai pra lá, o mesmo igual o outro. Tá vendo que ela tá ajustando? Agora invertendo esse ponto, ó. Vai ajustando. Ele vai ajustando até ficar no ponto ótimo pra ele. Não, então, mas... Quando ela fica burra, o que aconteceria? Ela pararia ali de... Ela pararia ali de... Não, não acontece muito isso não. Ela passa do sol, vamos falar assim. Ah, então pode vir acontecendo ela aqui? Pode, pode ser. Pode acontecer bonito, né? Pode acontecer bonito. Ela vai apanhar aí, ó. Você tá rodando no total. Vai apanhar. Aí ele vem descendo, ela vai subir. Mas ela chega. Amor, ela chega. Tá? Eu já caminhei com ela agora. Voltei um tempo. Não morri, né? Tá vendo, pessoal? É bonito de ver. É bonito de ver nas outras. Mas nem deu tempo, ó. Eu exagerei com a cortada aqui, né? Lindo, pessoal. Me mostra ou não? Bonito. Bonito, né? Eu tô até me sonhando com isso. Tá? Olha lá. Olha o que acontece. Quando a... A taxa de risada fica alta. Tá vendo que a barra tá pulando assim? Pelo amor de Deus. Ela não tá aí de vargazinho subindo. Ela tá saltando. Por quê? Ela tá fazendo justamente isso. Quando ela aprende muito rápido. Tá aqui um vale. Ela tá aqui. Ele salta pra cá. Salta pra cá. Salta pra cá e entra. Entendeu? Às vezes pula pra cá e vai. Quando a taxa é muito alta. Ele começa a pular. Então o que aconteceu é isso. Ele ficou fazendo assim, ó. O gráfico pra baixo e pra cima toda hora. Então o que acontece é esses saltos que tá aí. Tá ok? Ela tá aprendendo ainda. Ela acabou de aprender já. Aumentar essa taxa significa aumentar... A velocidade de aprendizagem. Ah, tá. Mas isso não significa que vai ter mais erros? Porque vai ter, ó. Vai passar direto, então. Ah, tá. Então vai passar... Não, não. Entendeu? Você sabe mesmo como monta uma rede neural, tá vendo? Você vai tirando os conceitos dela. Por que que... Por que que eu tenho que usar uma taxa menor? Por que que eu tenho que fazer isso? Porque o cara que só usa... Ele sabe usar uma ferramenta. Sabe usar o Excel da vida. Ponto. Ele não sabe se... Se apertar, ela não sai. Se apertar, chama a mãe. Depois ela não dupla aí, né? Ela vai até... Porque ela vai até... Ela vai até brigando aí. Aí ela... Tá na hora de parar é porque ela aprendeu tudo. Porque aprendeu tudo. Como é que eu acho que a linha que ela aprendeu tudo nesse mundo é de linhas? Não, é que eu tento fazer pra sacanear. Ou seja, é dar um clear aqui e limpar. Aí ficou só a última? É. Você deve dar um clear aí e deixar ela limpando. Ela ajustando aí, ó. Olha lá. Ajustando. Olha que bonito. É bonito, mas... Olha ali, perdendo. Agora tem que subir aqui. A gente vira pra lá. Tô jogando no celular todo. Ah, não. Tô jogando no celular todo. Por quê? Por que não? Por que não? Por exemplo. Ô, ô. O Flo perdeu. Ele perdeu. Isso aconteceu. Isso aconteceu. Tem que ter acesso nesse gráfico aqui também pra gente ver. Java tem coisa pra onde, filho? Só puxar o Wii? É. Tem aqui. É TKZ? Ele só não tem a ferramenta ali que te mostra as variáveis, né? Não. Esse bem não. Porque aqui, igual eu te falei, eu uso ela pra seguir. Então tem tudo que tá acontecendo. Até a memória que tá sendo usada. O resto tem tudo no Java mesmo? Tem tudo no Java. Tem tudo. Tem tudo. Beleza, turma? Lindo? Você já mostrou? O que que eu vou fazer? Eu vou deixar salvo pra vocês agora na área. Conselho que é amigo. Copinha. Chega... Se for que quiser mexer agora, tem outro minutinho pra acabar a aula. Viu que eu falei bastante? Salve. Eu mandei te falar muito, salve. É... Tá apanhando aqui mesmo aqui na hora, turma. É... O que que eu conselho vocês fazerem? Eu vou voltar aqui pra posição do percepto. Dá aula. Tá bem... Começa agora aqui, ó. Vou voltar pra cá, ó. Tá? Beleza? Ó. Beleza? Ó. Tá salvo, pessoal? Não, ó. Tá dentro do Júnior. Por que ele tá Júnior? Porque eles colocaram o Júnior lá até hoje. Aqui, ó. Tá? Dentro de A. Percepto, certo? Eu preciso conseguir aqui. Isso aqui eu vou deletar. Tá dentro do Fio. Beleza? Tá, isso aqui mesmo. Beleza, ó? Abaixa o arquivinho, pessoal. Vai lá e mexe, ó. Vamos fazer uma porta ordem. Faz a ordem aqui, ó. E vê o que acontece, ó. Tá? Já deu... Deixa ele treinar aí. Fazer... Se tiver que fazer aí. Tá rodando aqui. Beleza? Beleza. Deixa eu fechar isso aqui tudo. É... Se você não quiser aqui plotar gráfico ou nada, você pode vir aqui e tal, ó. E fazer isso aqui, tá? Ó. Ali gera aí... Sem gráfico, sem nada. Eu só vou mostrar pra você o W. Salva, salva. É... Vou liberar vocês. Vamos fazer chamada. Espera um pouquinho. É... Cadê? Dez a zero. Encerrar com ostras. Beleza? Funciona bem? É uma chamadinha básica. Acho que pode, né? Não, dá pra eu... Eu gostaria que você tomasse... Não. Puxa lá. Tá vendo aí? É. Até o final do semestre eu tenho seis... Tenho seis... Não. Seis sem... Seis meses pra guardar coisa dentro do álbum. Estudos e quadros. Você pega uma matéria que ninguém é rápido. Que é a Z.A.D. Você joga tudo lá. Até o consumo do oceano não aumenta. Anotado. Um exemplo. É filha da puta? É mesmo. Agora eu até bloqueava. Flux. Quanto que você cobraria pra ter um armazenamento grátis? Quanto que você pagaria pra ter um armazenamento grátis? No mês, assim. Não, não, não. É porque... Não, não. É não. É porque eu pago mensalidade. Então não é grátis, né? É que o Ava... Você pode jogar tudo isso de pá lá dentro durante seis meses, né? Antes deles pagarem tudo. Então é meio que grátis... Você usar o dinheiro de bordo pra gastar. Manda aonde, pô? Cai. Eduardo Ferrado. Eduardo Martins. Fernando Garcia. Oi? Fernando tá aqui? Ah, lascou. Ferrado todo mundo inteiro. Gabriel Custódio. Igor Taia. Quase errado. Eu quase errado. Luiz Gustavo. Luiz Gustavo. Bombaco. Não quero que fique vendo, assim. Matheus. Mala dos malas. Não, malas. Matheus tá aqui, ué. Matheus. Matheus. Por que não tem o Matheus vindo pra sala? O Cardoso? O Cardoso, é o tatuado. É, o Hermano. Aqui, ó. Faz chamada, meu filho. Ué, mas eu vou saber? Eu tô a falta de você. Como assim? É o quê? Tá a falta de você. Culosada tá aí. Matheus Cunha. Matheus Rosa tá aqui. Miguel Freitas. Mikael tá aí. Nicolás Tesla. Cadê? Não, mas são japoneses a vez. Pabllo Henrique. Riano. Ricardo Henrique. O italiano. Sérgio Roberto. Errava agora, calma. Quando eu chegar em casa eu mando lá de baixo. Vitor Mendes. Vinícius Mendes. Decorado. Melgles. Não, ele colocou no contador. Mas eu tô mandando o meu app pra baixar no contador em casa. A gente tem que baixar todos os apps. Se o cara não usa o app não vai fazer nada. O que é isso? Não, calma, o que é isso? Ah, os cursos. Ele deixa essa roupa? Que seria melhor se ele não tivesse um ponto ou alguma coisa. Ele deixa essa roupa. Ah, então funciona com títulos? Orlando Alves. É. Alt255. Só isso? Direito de flores. Não, acho que ele vai pegar tudo. Você vai pegar tudo. Eu não sei se eu vou pegar as flores ou não. Então você já perde. Cara, são 40 vagas. Pega logo, que liberou hoje. Ah, não sei. Hoje e amanhã. Não, você vem cá e já faz ele. Se a gente pôr as formas dos grupos, faz mais gente. Já ajuda, já ajuda. Vai virar uma parte diferente. Que delícia. Ele aceita, mano. É um bug do Alves. Vamos reportar. Alt255. Representa o cara que ele não fazia nada. Vou por lá mostrar o meu caso. O seu pai te falou? Oi? Não. Não comentou, não. Junho! Pode não, Junho. Não, faz isso não. Faz isso não. Vai ficar triste. Vou jogar foda. Vou jogar foda. Tem que começar a fazer assim já. Foda. A porcentagem. Se não vem, a porcentagem tá certa. Esse cheiro não é um. Aí, aí. Não é lógica boneana. Esquece o boneano. Filma a bolachinha dele. Filma a bolachinha dele. Filma a bolachinha dele. Mas eu tenho que atualizar. Pelo bem estar. O meu bem estar. É integral. Leite de coco, né? Pode pôr até o junto. Tem leite, eu não sei. Você entendeu que era gelactose? Acabou. Não, tem leite não. Tem leite não. Tem lactose. Tem. Tá bem no finalzinho ali. É bem no fundo, né? Não sei. Não, se der a minha ré, eu vou trocar. Por quê? Por deixar o fundo. Me envenenou. Mas é bom, né? Vou descobrir. É o que eu falava na época. Não tem destino não. Não é só nada. Meu Deus. Você desliga aqui. Olha aqui. O meu professor é sofredor. Tira a mão daí. O cara vem de Anamês. O outro vem de Brooksfield. E eu vim de balda estourada. Olha aí. Olha a criação, rapaz. Olha lá, japonês. Esse gato aqui quebra o cu, sabe?